{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words related to fastAI - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "**IMPORTANT**  _Not Important_\n",
    "\n",
    "* **Batchsize**\n",
    "* Image cleaner\n",
    "* **Learning rate**\n",
    "* **Data bunch**\n",
    "* **Convolution**\n",
    "* _Resnet_\n",
    "* _Tar targ.z_\n",
    "* **Normalization**\n",
    "* **Weight**\n",
    "* Learner\n",
    "* regex\n",
    "* **neural network**\n",
    "* **dataset**\n",
    "* pat\n",
    "* _unfreeze_\n",
    "\n",
    "* **epoch** : In Deep Learning, an epoch is a hyperparameter which is defined before training a model. One epoch is when an entire dataset is passed both forward and backward through the neural network only once. One epoch is too big to feed to the computer at once. So, we divide it in several smaller batches. We use more than one epoch because passing the entire dataset through a neural network is not enough and we need to pass the full dataset multiple times to the same neural network. A batch is the total number of training examples present in a single batch and an iteration is the number of batches needed to complete one epoch. For example: If we divide a dataset of 2000 training examples into 500 batches, then 4 iterations will complete 1 epoch. If we have 10.000 images dans le data set, and a batch size of 200, then an epoch should contain 10.000/200 = 50 iterations.\n",
    "\n",
    "* cycle learning\n",
    "* **loss** : By training neural networks, we essentially mean we are minimising a loss function. The value of this loss function gives us a measure how far from perfect is the performance of our network on a given dataset. Our goal is to find the particular value of weight for which the loss is minimum. Such a point is called a minima for the loss function.\n",
    "* **valid / train /test** : The construction of algorithms that can learn from and make predictions on data, needs to building a mathematical model from input data. The data used to build the final model usually comes from multiple datasets. In particular, three data sets are commonly used in different stages of the creation of the model: traning, validation and test set (optional). The model is initially fit on a **training dataset**,that is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model is trained on the training dataset using a supervised learning method. The current model is run with the training dataset and produces a result, which is then compared with the target, for each input vector in the training dataset. Successively, the fitted model is used to predict the responses for the observations in a second dataset called the **validation dataset**. The validation dataset provides an unbiased evaluation of a model fit on the training dataset while tuning the model's hyperparameters. Finally, the **test dataset** is an (optional) dataset used to provide an unbiased evaluation of a final model fit on the training dataset.When the data in the test dataset has never been used in training (for example in cross-validation), the test dataset is also called a holdout dataset.\n",
    "* **data augmentation** : Data augmentation means increasing the number of data points. In terms of images, it may mean that increasing the number of images in the dataset. In terms of traditional row/column format data, it means increasing the number of rows or objects. But why? We do not have infinite amount of data. The more the data, the better our ML models will be, in principle. But every data collection process is associated with a cost. This cost can be in terms of dollars, human effort, computational resources and off course time consumed in the process.\n",
    "* **depth** : In Deep Neural Networks the depth refers to how deep the network is, so how many (type of) layers we should stack\n",
    "* kaggle : Kaggle is an online community of data scientists and machine learners, owned by Google LLC. Kaggle allows users to find and publish data sets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges. Kaggle got its start by offering machine learning competitions and now also offers a public data platform, a cloud-based workbench for data science, and short form AI education. On 8 March 2017, Google announced that they were acquiring Kaggle.[1][2]\n",
    "* Neuron : The basic unit of computation in a neural network is the neuron, often called a node or unit. It receives input from some other nodes, or from an external source and computes an output. Each input has an associated weight (w), which is assigned on the basis of its relative importance to other inputs. Artificial Neural Networks do not work exactly the same as biological ones. An artificial neuron has 3 main parts: the input layer, the hidden layer, and the output layer. In terms of neurons, the input layer is your sensory neurons (your 5 senses), the output layer is your motor neurons (your mobility and actions), and the “hidden” layer is your interneurons, where the thinking and processing happens (inside your brain). I should point out that you can have n hidden layers; if you think about your brain, you have hundreds of millions of neurons that process in between your input and your output. You may have heard of the term deep learning: this implies multiple hidden layers, hidden because they’re not part of the visible output.\n",
    "* single hidden layer : Usually Neural Network comprises of three types of layer : Input Layer, Hidden Layer and Output layer. Initially in the development phase hidden layer was not used in the problems having linearly separable domain. But when any function that contains a continuous mapping from one finite space to another, one has to make use of single hidden layer. Biological neurons are fairly useless all by themselves, but a single artificial neuron can make quite a large amount of progress.\n",
    "* **confusion matrix** : A confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).\n",
    "* **error_rate** : The error basically signifies how well your network is performing on a certain (training/testing/validation) set. Having a low error is good, will having a higher error is certainly bad. The error is calculated through a loss function,\n",
    "* _heatmap_ : generating a heat map of all the feature variables — feature variables as row headers and column headers, and the variable vs itself on the diagonal— is extremely powerful way to visualize relationships between variables in high dimensional space (for example, a correlation matrix with heat map coloring).\n",
    "* **accuracy** : Once you have a model, it is important to check if your model is performing well on unseen examples that you have not used for training the model. Your accuracy generally depends on how much bias you are adding to the algorithms. Bias are of two types 1) internal bias (totally depends on the spaces on which our model is mounted) 2) external bias (bias induced by human interventions in the algorithm, example initialisation of parameters, randomly assigned there values etc..)\n",
    "* _metrics : Evaluating your machine learning algorithm is an essential part of any project. Your model may give you satisfying results when evaluated using a metric say accuracy_score but may give poor results when evaluated against other metrics such as logarithmic_loss or any other such metric. Other examples of metrics to evaluate algorithm performance : Classification Accuracy, Logarithmic Loss, Confusion Matrix, Area under Curve, F1 Score, Mean Absolute Error, Mean Squared Error.\n",
    "* _random seed_ : A random seed (or seed state, or just seed) is a number (or vector) used to initialize a pseudorandom number generator (an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers). The most common part where it will be used in deep learning is in the initialization of weights before training; the best ways currently known to do that involve randomness.\n",
    "* _data classes_ : Not all data is perfect. In fact, you’ll be extremely lucky if you ever get a perfectly balanced real-world dataset. Most of the time, your data will have some level of **class imbalance**, which is when each of your classes have a different number of examples. Why do we want our data to be balanced? Before committing time to any potentially lengthy task in a Deep Learning project, it’s important to understand why we should do it so that we can be sure it’s a valuable investment. Class balancing techniques are only really necessary when we actually care about the minority classes.\n",
    "* **CNN /convLearner** : In neural networks, Convolutional neural network (ConvNets or CNNs) is one of the main categories to do images recognition, images classifications. CNN image classifications takes an input image, process it and classify it under certain categories (Eg., Dog, Cat, Tiger, Lion). Computers sees an input image as array of pixels and it depends on the image resolution. Based on the image resolution, it will see h x w x d( h = Height, w = Width, d = Dimension ).\n",
    "* AlexNet : AlexNet is the name of a convolutional neural network that  competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. It is considered one of the most influential papers published in computer vision, having spurred many more papers published employing CNNs and GPUs to accelerate deep learning. As of 2018, the Alexnet paper has been cited over 30,000 times.\n",
    "* ImageNet : The ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories.  Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes. \n",
    "* **Model**\n",
    "* **Computer vision**\n",
    "* top loss\n",
    "* RNN\n",
    "* PyTorch\n",
    "* classification\n",
    "* TenserFlow\n",
    "* Fast AI\n",
    "* **Architecture**\n",
    "* _Sample_ : Echantillon de données\n",
    "* Layers\n",
    "* _Perceptron\n",
    "* **OverFitting**\n",
    "* **UnderFitting**\n",
    "* **Training / Testing**\n",
    "* _Cross Validation_ : La validation croisée1 (« cross-validation ») est, en ML, une méthode d’estimation de fiabilité d’un modèle fondé sur une technique d’échantillonnage. Supposons posséder un modèle statistique avec un ou plusieurs paramètres inconnus, et un ensemble de données d'apprentissage sur lequel on peut entraîner le modèle. Le processus d'apprentissage optimise les paramètres du modèle afin que celui-ci corresponde aux données le mieux possible. Si on prend ensuite un échantillon de validation indépendant issu de la même population d'entraînement, il s'avérera en général que le modèle ne réagit pas aussi bien à la validation que durant l'entraînement : on parle parfois de surapprentissage. La validation croisée est un moyen de prédire l'efficacité d'un modèle sur un ensemble de validation hypothétique lorsqu'un ensemble de validation indépendant et explicite n'est pas disponible. \n",
    "* Regression\n",
    "* **Gradient Descent** : In machine learning, we use gradient descent to update the parameters of our model (parameters refer to coefficients in linear regression and weights in neural networks). Gradient descent is is an optimization algorithm used to minimize a cost function J(W) parameterized by a model parameters W. The gradient (or derivative) tells us the incline or slope of the cost function. Hence, to minimize the cost function, we move in the direction opposite to the gradient. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill, a local minimum.\n",
    "* **Transfer Learning** : Transfer learning is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.[1] For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.\n",
    "* **Label**\n",
    "* **Prediction**\n",
    "* **Supervised / unsupervized**\n",
    "* **Validation data**\n",
    "* **Fine Tunning**\n",
    "* Random Forest\n",
    "* _Fine Grained Classification_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
