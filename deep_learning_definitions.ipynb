{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words related to fastAI - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "**IMPORTANT**  _Not Important_\n",
    "\n",
    "* **Batchsize** :\n",
    "Taille de l'échantillon de données que l'on fournit à analyser au GPU en tant que paramêtres d'un model deep learning\n",
    "ex: batch size 64 signifie qu'on fournit 64 images à analyser d'un coup au modèle\n",
    "Si l'on a pas assez de GPU, il faut réduire la taille du batch\n",
    "\n",
    "* ImageCleaner: permet de nettoyer le dataset des images qui n'y ont pas leur place.\n",
    "\n",
    "```ImageCleaner(dataset, fns_idxs, path, batch_size:int=5, duplicates=False)```\n",
    "* **Learning rate**: détermine la vitesse à laquelle on veut updater le poids des paramêtres. Le taux d'apprentissage doit être assez haut afin qu'il ne prenne pas des heures à converger, mais assez bas afin qu'il puisse trouver les minimums locaux\n",
    "\n",
    "* **Data bunch**\n",
    "Object utilisé dans un learner pour entrainer un modèle. C'est une classe générique qui peut prendre en paramêtre n'importe quel type de fastai Dataset ou DataLoader \n",
    "\n",
    "* **Convolution** Opération par laquelle deux fonctions sont mises dans un rapport suggérant une sorte d'enroulement de l'une sur l'autre.\n",
    "\n",
    "* _Resnet_\n",
    "\n",
    "* _Tar / tar.gz_\n",
    "\n",
    "* **Normalization** : procédé selon lequel avec un data set donné, on soustrait de chaque élément la valeur moyenne du data set que l'on  divise ensuite par la variance du data set. Cela permet de mettre toutes les différentes valeurs à la même \"échelle\". \n",
    "La raison dérière cette action vient du fait que certains inputs ayant des valeurs extrèmes peuvent rendre instable le réseau de neurones.\n",
    "\n",
    "* **Weights** : le \"poids\" représente la force d'une connection entre deux neurones. Si le poids du neurone 1 vers neurone 2 est élevé, cela signifie que neurone 1 a une grande influence sur neurone 2. \n",
    "\n",
    "* Learner: Il s'agit d'une class disponible dans fastAi. Le but principal d'un Learner est d'entrainer un modèle. \n",
    "\n",
    "* regex (regular expression): chaîne de caractères, qui décrit, selon une syntaxe précise, un ensemble de chaînes de caractères possibles. \n",
    "\n",
    "* **neural network**: série d'algorythmes dont le but est de reconnaitre les relations sous-jacentes des données d'un data-set au travers d'une approche qui immite le comportement du cerveu humain\n",
    "\n",
    "* **dataset**: ensemble de données. Il s'agit du contenu d'une table de base de données, ou d'une matrice de données, ou chaque colonne représente un type de variable et chaque ligne représente un élément du dataset en question\n",
    "\n",
    "* _unfreeze_ : methode pour \"dégeler\" tout le modèle. Chaque groupe de layer est set à _trainable_\n",
    "\n",
    "* **epoch** (training epoch):  Nombre de fois que le training dataset passe par le modèle\n",
    "\n",
    "* cycle learning\n",
    "\n",
    "* **loss**: By training neural networks, we essentially mean we are minimising a loss function. The value of this loss function gives us a measure of how far from perfect is the performance of our network on a given dataset. Our goal is to find the particular value of weight for which the loss is minimum. Such a point is called a minima for the loss function.\n",
    "\n",
    "* **valid / train / test** :The construction of algorithms that can learn from and make predictions on data, needs to building a mathematical model from input data. The data used to build the final model usually comes from multiple datasets. In particular, three data sets are commonly used in different stages of the creation of the model: traning, validation and test set (optional). The model is initially fit on a training dataset,that is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model is trained on the training dataset using a supervised learning method. The current model is run with the training dataset and produces a result, which is then compared with the target, for each input vector in the training dataset. Successively, the fitted model is used to predict the responses for the observations in a second dataset called the validation dataset. The validation dataset provides an unbiased evaluation of a model fit on the training dataset while tuning the model's hyperparameters. Finally, the test dataset is an (optional) dataset used to provide an unbiased evaluation of a final model fit on the training dataset.When the data in the test dataset has never been used in training (for example in cross-validation), the test dataset is also called a holdout dataset.\n",
    "\n",
    "* **data augmentation**: Data augmentation means increasing the number of data points. In terms of images, it may mean that increasing the number of images in the dataset. In terms of traditional row/column format data, it means increasing the number of rows or objects. But why? We do not have infinite amount of data. The more the data, the better our ML models will be, in principle. But every data collection process is associated with a cost. This cost can be in terms of dollars, human effort, computational resources and off course time consumed in the process.\n",
    "\n",
    "* **depth**:  In Deep Neural Networks the depth refers to how deep the network is, so how many (type of) layers we should stack\n",
    "\n",
    "* Neuron: The basic unit of computation in a neural network is the neuron, often called a node or unit. It receives input from some other nodes, or from an external source and computes an output. Each input has an associated weight (w), which is assigned on the basis of its relative importance to other inputs. Artificial Neural Networks do not work exactly the same as biological ones. An artificial neuron has 3 main parts: the input layer, the hidden layer, and the output layer. In terms of neurons, the input layer is your sensory neurons (your 5 senses), the output layer is your motor neurons (your mobility and actions), and the “hidden” layer is your interneurons, where the thinking and processing happens (inside your brain). I should point out that you can have n hidden layers; if you think about your brain, you have hundreds of millions of neurons that process in between your input and your output. You may have heard of the term deep learning: this implies multiple hidden layers, hidden because they’re not part of the visible output.\n",
    "\n",
    "* path\n",
    "\n",
    "* **confusion matrix** : A confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).\n",
    "\n",
    "* **error_rate**: The error basically signifies how well your network is performing on a certain (training/testing/validation) set. Having a low error is good, will having a higher error is certainly bad. The error is calculated through a loss function,\n",
    "\n",
    "* _heatmap_: generating a heat map of all the feature variables — feature variables as row headers and column headers, and the variable vs itself on the diagonal— is extremely powerful way to visualize relationships between variables in high dimensional space (for example, a correlation matrix with heat map coloring).\n",
    "\n",
    "* **accuracy**: Once you have a model, it is important to check if your model is performing well on unseen examples that you have not used for training the model. Your accuracy generally depends on how much bias you are adding to the algorithms. Bias are of two types 1) internal bias (totally depends on the spaces on which our model is mounted) 2) external bias (bias induced by human interventions in the algorithm, example initialisation of parameters, randomly assigned there values etc..)\n",
    "\n",
    "* _metrics :Evaluating your machine learning algorithm is an essential part of any project. Your model may give you satisfying results when evaluated using a metric say accuracyscore but may give poor results when evaluated against other metrics such as logarithmic_loss or any other such metric. Other examples of metrics to evaluate algorithm performance : Classification Accuracy, Logarithmic Loss, Confusion Matrix, Area under Curve, F1 Score, Mean Absolute Error, Mean Squared Error.\n",
    "\n",
    "* single hidden layer:  Usually Neural Network comprises of three types of layer : Input Layer, Hidden Layer and Output layer. Initially in the development phase hidden layer was not used in the problems having linearly separable domain. But when any function that contains a continuous mapping from one finite space to another, one has to make use of single hidden layer. Biological neurons are fairly useless all by themselves, but a single artificial neuron can make quite a large amount of progress.\n",
    "\n",
    "* _random seed_: A random seed (or seed state, or just seed) is a number (or vector) used to initialize a pseudorandom number generator (an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers). The most common part where it will be used in deep learning is in the initialization of weights before training; the best ways currently known to do that involve randomness.\n",
    "\n",
    "* _data classes_ : Not all data is perfect. In fact, you’ll be extremely lucky if you ever get a perfectly balanced real-world dataset. Most of the time, your data will have some level of class imbalance, which is when each of your classes have a different number of examples. Why do we want our data to be balanced? Before committing time to any potentially lengthy task in a Deep Learning project, it’s important to understand why we should do it so that we can be sure it’s a valuable investment. Class balancing techniques are only really necessary when we actually care about the minority classes.\n",
    "\n",
    "* **CNN /convLearner** :  In neural networks, Convolutional neural network (ConvNets or CNNs) is one of the main categories to do images recognition, images classifications. CNN image classifications takes an input image, process it and classify it under certain categories (Eg., Dog, Cat, Tiger, Lion). Computers sees an input image as array of pixels and it depends on the image resolution. Based on the image resolution, it will see h x w x d( h = Height, w = Width, d = Dimension ).\n",
    "\n",
    "* AlexNet : AlexNet : AlexNet is the name of a convolutional neural network that competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. It is considered one of the most influential papers published in computer vision, having spurred many more papers published employing CNNs and GPUs to accelerate deep learning. As of 2018, the Alexnet paper has been cited over 30,000 times.\n",
    "\n",
    "* ImageNet : The ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories. Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes.\n",
    "\n",
    "* **Model** : Machine learning model training\n",
    "\n",
    "* **Computer vision** : Reconnaissance visuelle, analyser, traiter, comprendre, des images. include face recognition and indexing, photo stylization or machine vision in self-driving cars\n",
    "\n",
    "* top loss:  Meilleurs perte, Loss value implies how well or poorly a certain model behaves after each iteration of optimization. Loss is often used in the training process to find the \"best\" parameter values for your model (e.g. weights in neural network). It is what you try to optimize in the training by updating weights.\n",
    "\n",
    "* RNN: Recurent neural network, reseau de neurone recurent. Un réseau de neurones récurrents est constitué d'unités (neurones) interconnectés interagissant non-linéairement et pour lequel il existe au moins un cycle dans la structure. Les unités sont reliées par des arcs (synapses) qui possèdent un poids.\n",
    "\n",
    "* PyTorch: Bibliothèque logicielle Python d'apprentissage machine développée par Facebook. \n",
    "    Qui permet de manipuler des tenseurs (tableaux multidimensionnels), de les échanger facilement avec Numpy et d'effectuer des calculs efficaces sur CPU ou GPU (par exemple, des produites de matrices ou des convolutions);\n",
    "    Egalement de calculer des gradients pour appliquer facilement des algorithmes d'optimisation par descente de gradient.\n",
    "\n",
    "* classification: Attribuer une classe ou catégorie à chacune des observations d’un jeu de données . Se fait, une fois les données récupérées.\n",
    "\n",
    "* TenserFlow : Outil de Google, d'apprentisage automatique, basé sur l'apprentissage profnond, c'est l'un des outils les plus utlisé dans le monde pour le machine learning.\n",
    "\n",
    "* Fast AI: est une biblioteque, et un Algorithme basé sur python et pytorch de machine learning\n",
    "\n",
    "* **Architecture**: On utilise le mot architecture pour décrire les différents type d'algorithmes, utilisé dans le deep learning, il en existe 3 principaux, le Réseau de neurone, les Réeaux de neurones non convolutifs (CNN), les reseaux de neurone recurent, les auto encodeurs\n",
    "\n",
    "* _Sample_ : Echantillon de dataset, comme une photo, que le donne un a un, a la machine.\n",
    "\n",
    "* **Layers**\n",
    "A différencier entre input Layer , hidden layer et output layers. A layer = une couche. Input Layer est la première couche du réseau de neurones. Il prend en input des valeurs et les passe à la prochaine couche. Il ne fait aucune opération sur ce valeurs, et n'a ni weights, ni biases \n",
    "\n",
    "* _Perceptron_ : C'est un algorithme d'apprentissage supervisé de classifieurs binaires. C'est un classifieur linéaire.\n",
    "\n",
    "* **OverFitting / UnderFitting** : Définition de statistiques, de sur-interpretation ou sous-interpretation, le sur-apprentisssage résulte souvent d'une trop grande liberté dans le choix du modèle. Une méthode permettant d'éviter le surapprentissage est d'utiliser une forme de régularisation. On pénalise les valeurs extrêmes des paramètres, car ces valeurs correspondent souvent à un surapprentissage. Pour détecter un surapprentissage, on sépare les données en deux : l'ensemble d'apprentissage et l'ensemble de validation. L'ensemble d'apprentissage comme son nom l'indique permet de faire évoluer les poids du réseau de neurones. L'ensemble de validation n'est pas utilisé pour l'apprentissage mais permet de vérifier la pertinence du réseau avec des échantillons qu'il ne connait pas.\n",
    "\n",
    "* **UnderFitting** : On peut parler de surapprentissage si l'erreur de prédiction du réseau sur l'ensemble d'apprentissage diminue alors que l'erreur sur la validation augmente de manière significative. Cela signifie que le réseau continue à améliorer ses performances sur les échantillons d'apprentissage mais perd son pouvoir de prédiction sur ceux provenant de la validation. \n",
    "[1_JZbxrdzabrT33Yl-LrmShw.png](attachment:1_JZbxrdzabrT33Yl-LrmShw.png)\n",
    "\n",
    "* **Training / Testing**: 1er dataset; L'entrainement consiste à faire tourner un set d'exemple qui servira pour parametrer les poids du models 2ème le dataset de validation, utilisé pour entrainer les algorithmes et décidé de ccelui a utiliser 3émé dataset de teste, est indépendant du dataset validation, il est mieux calibré que celui-ci et sert à tester les performance de l'architecture choisie.\n",
    "\n",
    "* _Cross Validation_ : La validation croisée1 (« cross-validation ») est, en ML, une méthode d’estimation de fiabilité d’un modèle fondé sur une technique d’échantillonnage. Supposons posséder un modèle statistique avec un ou plusieurs paramètres inconnus, et un ensemble de données d'apprentissage sur lequel on peut entraîner le modèle. Le processus d'apprentissage optimise les paramètres du modèle afin que celui-ci corresponde aux données le mieux possible. Si on prend ensuite un échantillon de validation indépendant issu de la même population d'entraînement, il s'avérera en général que le modèle ne réagit pas aussi bien à la validation que durant l'entraînement : on parle parfois de surapprentissage. La validation croisée est un moyen de prédire l'efficacité d'un modèle sur un ensemble de validation hypothétique lorsqu'un ensemble de validation indépendant et explicite n'est pas disponible.\n",
    "\n",
    "* **Regression**: ensemble de méthodes statistiques très utilisées pour analyser la relation d'une variable par rapport à une ou plusieurs autres.\n",
    "\n",
    "* **Gradient Descent** :  In machine learning, we use gradient descent to update the parameters of our model (parameters refer to coefficients in linear regression and weights in neural networks). Gradient descent is is an optimization algorithm used to minimize a cost function J(W) parameterized by a model parameters W. The gradient (or derivative) tells us the incline or slope of the cost function. Hence, to minimize the cost function, we move in the direction opposite to the gradient. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill, a local minimum.\n",
    "\n",
    "* **Transfer Learning** : Transfer learning is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.[1] For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.\n",
    "\n",
    "* **Label**\n",
    "\n",
    "* **Prediction**\n",
    "\n",
    "* **Supervised / unsupervized**\n",
    "\n",
    "* **Validation data**\n",
    "\n",
    "* **Fine Tunning**\n",
    "\n",
    "* Random Forest\n",
    "\n",
    "* _Fine Grained Classification_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
